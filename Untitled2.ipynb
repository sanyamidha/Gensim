{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [\n",
    "   \"CNTK formerly known as Computational Network Toolkit\",\n",
    "   \"is a free easy-to-use open-source commercial-grade toolkit\",\n",
    "   \"that enable us to train deep learning algorithms to learn like the human brain.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(27 unique tokens: ['CNTK', 'Computational', 'Network', 'Toolkit', 'as']...)\n"
     ]
    }
   ],
   "source": [
    "text_tokens = [[text for text in doc.split()] for doc in doc]\n",
    "dict_LoS = corpora.Dictionary(text_tokens)\n",
    "print(dict_LoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CNTK': 0, 'Computational': 1, 'Network': 2, 'Toolkit': 3, 'as': 4, 'formerly': 5, 'known': 6, 'a': 7, 'commercial-grade': 8, 'easy-to-use': 9, 'free': 10, 'is': 11, 'open-source': 12, 'toolkit': 13, 'algorithms': 14, 'brain.': 15, 'deep': 16, 'enable': 17, 'human': 18, 'learn': 19, 'learning': 20, 'like': 21, 'that': 22, 'the': 23, 'to': 24, 'train': 25, 'us': 26}\n"
     ]
    }
   ],
   "source": [
    "print(dict_LoS.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-6-7de42c31fe2e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-7de42c31fe2e>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "dict_STF = corpora.Dictionary(\n",
    "   simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\n",
    ")\n",
    "dict_STF = corpora.Dictionary(text_tokens)\n",
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-7-aab5239856ad>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-aab5239856ad>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "dict_STF = corpora.Dictionary(\n",
    "   simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\n",
    ")\n",
    "dict_STF = corpora.Dictionary(text_tokens)\n",
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (3, 1), (4, 2)], [(0, 2), (3, 3), (5, 2), (6, 1), (7, 2), (8, 1)]]\n",
      "[[('are', 1), ('hello', 1), ('how', 1), ('you', 1)], [('how', 1), ('you', 1), ('do', 2)], [('are', 2), ('you', 3), ('doing', 2), ('hey', 1), ('what', 2), ('yes', 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "doc_list = [\n",
    "   \"Hello, how are you?\", \"How do you do?\", \n",
    "   \"Hey what are you doing? yes you What are you doing?\"\n",
    "]\n",
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "print(BoW_corpus)\n",
    "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
    "print(id_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-9-aab5239856ad>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-aab5239856ad>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "dict_STF = corpora.Dictionary(\n",
    "   simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\n",
    ")\n",
    "dict_STF = corpora.Dictionary(text_tokens)\n",
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-10-c52b0254b98a>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-c52b0254b98a>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "doc_tokenized = [\n",
    "   simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\n",
    "]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "print(BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-11-c52b0254b98a>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-c52b0254b98a>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "doc_tokenized = [\n",
    "   simple_preprocess(line, deacc =True) for line in open(‘doc.txt’, encoding=’utf-8’)\n",
    "]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "print(BoW_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c496bc82f265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m    ]\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt_corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 't_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "processed_corpus = [\n",
    "   [\n",
    "      word for word in document.lower().split() if word not in stoplist\n",
    "   ]\n",
    "\tfor document in t_corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_corpus = [\n",
    "   \"A survey of user opinion of computer system response time\",\n",
    "   \"Relation of user perceived response time to error measurement\",\n",
    "   \"The generation of random binary unordered trees\",\n",
    "   \"The intersection graph of paths in trees\",\n",
    "   \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-15-f06ba5a081aa>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-f06ba5a081aa>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    \"you can also find\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "t_corpus = [\n",
    "   \"CNTK formerly known as Computational Network Toolkit\", \n",
    "   \"is a free easy-to-use open-source commercial-grade toolkit\", \n",
    "   \"that enable us to train deep learning algorithms to learn like the human brain.\", \n",
    "   \"You can find its free videos\", \n",
    "   \"you can also find \"\n",
    "]\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "processed_corpus = [\n",
    "   [word for word in document.lower().split() if word not in stoplist]\n",
    "   for document in t_corpus\n",
    "]\n",
    "frequency = defaultdict(int)\n",
    "for text in processed_corpus:\n",
    "   for token in text:\n",
    "      frequency[token] += 1\n",
    "   processed_corpus = [\n",
    "      [token for token in text if frequency[token] > 1] \n",
    "      for text in processed_corpus\n",
    "   ]\n",
    "pprint.pprint(processed_corpus)\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)\n",
    "BoW_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(BoW_corpus)\n",
    "   from gensim import models\n",
    "   tfidf = models.TfidfModel(BoW_corpus)\n",
    "   doc_BoW = [(1,1),(3,1)]\n",
    "   print(tfidf[doc_BoW])\n",
    "   corpus_tfidf = tfidf[BoW_corpus]\n",
    "   for doc in corpus_tfidf:\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-16-32006cdd7fbe>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-32006cdd7fbe>\"\u001b[1;36m, line \u001b[1;32m30\u001b[0m\n\u001b[1;33m    from gensim import models\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "t_corpus = [\n",
    "   \"CNTK formerly known as Computational Network Toolkit\", \n",
    "   \"is a free easy-to-use open-source commercial-grade toolkit\", \n",
    "   \"that enable us to train deep learning algorithms to learn like the human brain.\", \n",
    "   \"You can find its free videos\", \n",
    "   \"you can also find \"\n",
    "]\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "processed_corpus = [\n",
    "   [word for word in document.lower().split() if word not in stoplist]\n",
    "   for document in t_corpus\n",
    "]\n",
    "frequency = defaultdict(int)\n",
    "for text in processed_corpus:\n",
    "   for token in text:\n",
    "      frequency[token] += 1\n",
    "   processed_corpus = [\n",
    "      [token for token in text if frequency[token] > 1] \n",
    "      for text in processed_corpus\n",
    "   ]\n",
    "pprint.pprint(processed_corpus)\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)\n",
    "BoW_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(BoW_corpus)\n",
    "   from gensim import models\n",
    "   tfidf = models.TfidfModel(BoW_corpus)\n",
    "   doc_BoW = [(1,1),(3,1)]\n",
    "   print(tfidf[doc_BoW])\n",
    "   corpus_tfidf = tfidf[BoW_corpus]\n",
    "   for doc in corpus_tfidf:\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['are', 1], ['hello', 1], ['how', 1], ['you', 1]]\n",
      "[['how', 1], ['you', 1], ['do', 2]]\n",
      "[['are', 2], ['you', 3], ['doing', 2], ['hey', 1], ['what', 2], ['yes', 1]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fb5fc826e894>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBoW_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmartirs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ntc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBoW_corpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecomal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "doc_list = [\n",
    "   \"Hello, how are you?\", \"How do you do?\", \n",
    "   \"Hey what are you doing? yes you What are you doing?\"\n",
    "]\n",
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "for doc in BoW_corpus:\n",
    "   print([[dictionary[id], freq] for id, freq in doc])\n",
    "import numpy as np\n",
    "tfidf = models.TfidfModel(BoW_corpus, smartirs='ntc')\n",
    "for doc in tfidf[BoW_corpus]:\n",
    "   print([[dictionary[id], np.around(freq,decomal=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-466d5d516508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "data = newsgroups_train.data\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "print(data_words[:4]) #it will print the data after prepared for stopwords\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "def remove_stopwords(texts):\n",
    "   return [[word for word in simple_preprocess(str(doc)) \n",
    "   if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "   return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "   [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "   texts_out = []\n",
    "   for sent in texts:\n",
    "      doc = nlp(\" \".join(sent))\n",
    "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "   return texts_out\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[\n",
    "   'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "])\n",
    "print(data_lemmatized[:4]) #it will print the lemmatized data.\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[:4]) #it will print the corpus we created above.\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n",
    "#it will print the words with their frequencies.\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "   corpus=corpus, id2word=id2word, num_topics=20, random_state=100, \n",
    "   update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyLDAvis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9e1628e71544>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyLDAvis' is not defined"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
